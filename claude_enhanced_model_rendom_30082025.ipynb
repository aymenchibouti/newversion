{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr3JanHua5lgRs7cJA0opt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/newversion/blob/master/claude_enhanced_model_rendom_30082025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "id": "DMismpC2tAFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW1XzrHCs7H8"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "COMPLETE ENHANCED MACHINE LEARNING PIPELINE FOR DROPOUT PREDICTION\n",
        "==================================================================\n",
        "\n",
        "This is the main execution script that combines all advanced techniques:\n",
        "- Data preprocessing and feature engineering\n",
        "- Multiple sampling strategies (SMOTE, ADASYN, Undersampling)\n",
        "- Variational Autoencoder for feature extraction\n",
        "- Comprehensive model comparison (RandomForest, XGBoost, LightGBM, etc.)\n",
        "- Advanced hyperparameter tuning\n",
        "- Model interpretation (SHAP, LIME, Permutation Importance)\n",
        "- Model persistence and deployment utilities\n",
        "\n",
        "Usage: python complete_pipeline.py\n",
        "\n",
        "Author: Enhanced ML Pipeline\n",
        "Date: 2024\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Import all required libraries\n",
        "try:\n",
        "    # Core ML libraries\n",
        "    from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
        "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.neural_network import MLPClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                               confusion_matrix, classification_report, roc_auc_score, roc_curve)\n",
        "\n",
        "    # Imbalanced data handling\n",
        "    from imblearn.over_sampling import SMOTE, ADASYN\n",
        "    from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "    # Advanced models\n",
        "    import xgboost as xgb\n",
        "    import lightgbm as lgb\n",
        "\n",
        "    # Model interpretation\n",
        "    try:\n",
        "        import shap\n",
        "        SHAP_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        print(\"Warning: SHAP not available. Install with: pip install shap\")\n",
        "        SHAP_AVAILABLE = False\n",
        "\n",
        "    try:\n",
        "        import lime\n",
        "        import lime.lime_tabular\n",
        "        LIME_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        print(\"Warning: LIME not available. Install with: pip install lime\")\n",
        "        LIME_AVAILABLE = False\n",
        "\n",
        "    # Deep learning for VAE\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        from tensorflow import keras\n",
        "        from tensorflow.keras import layers\n",
        "        TF_AVAILABLE = True\n",
        "    except ImportError:\n",
        "        print(\"Warning: TensorFlow not available. Install with: pip install tensorflow\")\n",
        "        TF_AVAILABLE = False\n",
        "\n",
        "    # Model persistence\n",
        "    import joblib\n",
        "    import pickle\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing required libraries: {e}\")\n",
        "    print(\"Please install required packages:\")\n",
        "    print(\"pip install scikit-learn imbalanced-learn xgboost lightgbm shap lime tensorflow joblib\")\n",
        "    sys.exit(1)\n",
        "\n",
        "class CompleteMLPipeline:\n",
        "    \"\"\"Complete enhanced machine learning pipeline\"\"\"\n",
        "\n",
        "    def __init__(self, data_path='model1_210_features_spliting.csv', output_dir='ml_results'):\n",
        "        self.data_path = data_path\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Initialize attributes\n",
        "        self.data = None\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.feature_names = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.results = {}\n",
        "        self.best_model = None\n",
        "        self.best_score = 0\n",
        "\n",
        "        # Create subdirectories\n",
        "        (self.output_dir / 'models').mkdir(exist_ok=True)\n",
        "        (self.output_dir / 'plots').mkdir(exist_ok=True)\n",
        "        (self.output_dir / 'reports').mkdir(exist_ok=True)\n",
        "\n",
        "        print(f\"ðŸš€ Enhanced ML Pipeline initialized\")\n",
        "        print(f\"ðŸ“ Output directory: {self.output_dir.absolute()}\")\n",
        "\n",
        "    def load_and_explore_data(self):\n",
        "        \"\"\"Load and perform initial data exploration\"\"\"\n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ“Š DATA LOADING AND EXPLORATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Load data\n",
        "        if not os.path.exists(self.data_path):\n",
        "            raise FileNotFoundError(f\"Data file not found: {self.data_path}\")\n",
        "\n",
        "        print(f\"Loading data from: {self.data_path}\")\n",
        "        self.data = pd.read_csv(self.data_path)\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"Dataset shape: {self.data.shape}\")\n",
        "        print(f\"Memory usage: {self.data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "        # Remove non-feature columns\n",
        "        cols_to_drop = ['username', 'course_id', 'enrollment_id']\n",
        "        existing_cols = [col for col in cols_to_drop if col in self.data.columns]\n",
        "        if existing_cols:\n",
        "            self.data = self.data.drop(columns=existing_cols)\n",
        "            print(f\"Removed columns: {existing_cols}\")\n",
        "\n",
        "        # Handle missing values\n",
        "        missing_before = self.data.isnull().sum().sum()\n",
        "        if missing_before > 0:\n",
        "            print(f\"Missing values found: {missing_before}\")\n",
        "            self.data = self.data.fillna(0)\n",
        "            print(\"Missing values filled with 0\")\n",
        "\n",
        "        # Verify target column\n",
        "        if 'dropout' not in self.data.columns:\n",
        "            raise KeyError(\"Target column 'dropout' not found\")\n",
        "\n",
        "        # Prepare features and target\n",
        "        self.X = self.data.drop(columns=['dropout'])\n",
        "        self.y = self.data['dropout']\n",
        "        self.feature_names = self.X.columns.tolist()\n",
        "\n",
        "        # Target distribution\n",
        "        target_counts = self.y.value_counts()\n",
        "        target_pct = self.y.value_counts(normalize=True) * 100\n",
        "\n",
        "        print(f\"\\\\nTarget distribution:\")\n",
        "        print(f\"  No dropout (0): {target_counts[0]:,} ({target_pct[0]:.1f}%)\")\n",
        "        print(f\"  Dropout (1): {target_counts[1]:,} ({target_pct[1]:.1f}%)\")\n",
        "        print(f\"  Imbalance ratio: {target_counts[0] / target_counts[1]:.1f}:1\")\n",
        "\n",
        "        # Create visualization\n",
        "        self._create_eda_plots()\n",
        "\n",
        "        return self.X, self.y\n",
        "\n",
        "    def _create_eda_plots(self):\n",
        "        \"\"\"Create exploratory data analysis plots\"\"\"\n",
        "        print(\"Creating EDA visualizations...\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        fig.suptitle('Exploratory Data Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Target distribution\n",
        "        target_counts = self.y.value_counts()\n",
        "        axes[0, 0].pie(target_counts.values, labels=['No Dropout', 'Dropout'],\n",
        "                      autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightcoral'])\n",
        "        axes[0, 0].set_title('Target Distribution')\n",
        "\n",
        "        # 2. Feature correlation with target\n",
        "        numeric_data = pd.concat([self.X, self.y], axis=1)\n",
        "        correlations = numeric_data.corr()['dropout'].drop('dropout').abs().sort_values(ascending=False)[:20]\n",
        "\n",
        "        axes[0, 1].barh(range(len(correlations)), correlations.values, color='skyblue')\n",
        "        axes[0, 1].set_yticks(range(len(correlations)))\n",
        "        axes[0, 1].set_yticklabels([name[:15] + '...' if len(name) > 15 else name for name in correlations.index], fontsize=8)\n",
        "        axes[0, 1].set_title('Top 20 Features Correlated with Dropout')\n",
        "        axes[0, 1].set_xlabel('Absolute Correlation')\n",
        "\n",
        "        # 3. Sample feature distributions\n",
        "        sample_features = self.X.columns[:6]  # First 6 features\n",
        "        for i, feature in enumerate(sample_features):\n",
        "            row, col = divmod(i + 2, 3)\n",
        "            if row < 2:\n",
        "                axes[row, col].hist(self.X[feature], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "                axes[row, col].set_title(f'{feature[:20]}...' if len(feature) > 20 else feature, fontsize=10)\n",
        "                axes[row, col].set_xlabel('Value')\n",
        "                axes[row, col].set_ylabel('Frequency')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / 'plots' / 'eda_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Correlation heatmap for top features\n",
        "        top_features = correlations.head(15).index.tolist() + ['dropout']\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        correlation_matrix = numeric_data[top_features].corr()\n",
        "\n",
        "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                   center=0, square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Correlation Heatmap - Top Features + Target')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / 'plots' / 'correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def prepare_datasets(self):\n",
        "        \"\"\"Prepare multiple datasets with different sampling strategies\"\"\"\n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ”„ DATASET PREPARATION WITH MULTIPLE SAMPLING STRATEGIES\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Scale features\n",
        "        print(\"Scaling features...\")\n",
        "        X_scaled = self.scaler.fit_transform(self.X)\n",
        "\n",
        "        # Initial train-test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_scaled, self.y, test_size=0.2, random_state=42, stratify=self.y\n",
        "        )\n",
        "\n",
        "        print(f\"Train set: {X_train.shape[0]:,} samples\")\n",
        "        print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "        datasets = {}\n",
        "\n",
        "        # 1. Original dataset (imbalanced)\n",
        "        datasets['original'] = {\n",
        "            'X_train': X_train,\n",
        "            'X_test': X_test,\n",
        "            'y_train': y_train,\n",
        "            'y_test': y_test,\n",
        "            'description': 'Original imbalanced dataset'\n",
        "        }\n",
        "        print(f\"âœ“ Original dataset prepared\")\n",
        "\n",
        "        # 2. SMOTE oversampling\n",
        "        try:\n",
        "            smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train) - 1))\n",
        "            X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "            datasets['smote'] = {\n",
        "                'X_train': X_train_smote,\n",
        "                'X_test': X_test,\n",
        "                'y_train': y_train_smote,\n",
        "                'y_test': y_test,\n",
        "                'description': 'SMOTE oversampled dataset'\n",
        "            }\n",
        "            print(f\"âœ“ SMOTE dataset prepared: {X_train_smote.shape[0]:,} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: SMOTE failed - {e}\")\n",
        "\n",
        "        # 3. ADASYN oversampling\n",
        "        try:\n",
        "            adasyn = ADASYN(random_state=42, n_neighbors=min(5, sum(y_train) - 1))\n",
        "            X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
        "            datasets['adasyn'] = {\n",
        "                'X_train': X_train_adasyn,\n",
        "                'X_test': X_test,\n",
        "                'y_train': y_train_adasyn,\n",
        "                'y_test': y_test,\n",
        "                'description': 'ADASYN oversampled dataset'\n",
        "            }\n",
        "            print(f\"âœ“ ADASYN dataset prepared: {X_train_adasyn.shape[0]:,} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: ADASYN failed - {e}\")\n",
        "\n",
        "        # 4. Random undersampling\n",
        "        try:\n",
        "            undersampler = RandomUnderSampler(random_state=42)\n",
        "            X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
        "            datasets['undersample'] = {\n",
        "                'X_train': X_train_under,\n",
        "                'X_test': X_test,\n",
        "                'y_train': y_train_under,\n",
        "                'y_test': y_test,\n",
        "                'description': 'Random undersampled dataset'\n",
        "            }\n",
        "            print(f\"âœ“ Undersampling dataset prepared: {X_train_under.shape[0]:,} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Undersampling failed - {e}\")\n",
        "\n",
        "        # Apply Variational Autoencoder if TensorFlow is available\n",
        "        if TF_AVAILABLE:\n",
        "            vae_datasets = self._apply_vae_feature_extraction(datasets)\n",
        "            datasets.update(vae_datasets)\n",
        "        else:\n",
        "            print(\"âš ï¸ TensorFlow not available, skipping VAE feature extraction\")\n",
        "\n",
        "        return datasets\n",
        "\n",
        "    def _apply_vae_feature_extraction(self, datasets, latent_dim=50):\n",
        "        \"\"\"Apply VAE for feature extraction\"\"\"\n",
        "        print(\"\\\\nðŸ§  Applying Variational Autoencoder for feature extraction...\")\n",
        "\n",
        "        vae_datasets = {}\n",
        "\n",
        "        for name, dataset in datasets.items():\n",
        "            if 'vae' in name:  # Skip already processed VAE datasets\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"  Processing {name} dataset with VAE...\")\n",
        "\n",
        "                # Build VAE\n",
        "                input_dim = dataset['X_train'].shape[1]\n",
        "                vae = self._build_vae(input_dim, latent_dim)\n",
        "\n",
        "                # Train VAE\n",
        "                history = vae.fit(\n",
        "                    dataset['X_train'], dataset['X_train'],\n",
        "                    epochs=50,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Extract features using encoder\n",
        "                encoder = keras.Model(vae.input, vae.get_layer('z_mean').output)\n",
        "                X_train_vae = encoder.predict(dataset['X_train'], verbose=0)\n",
        "                X_test_vae = encoder.predict(dataset['X_test'], verbose=0)\n",
        "\n",
        "                vae_datasets[f\"{name}_vae\"] = {\n",
        "                    'X_train': X_train_vae,\n",
        "                    'X_test': X_test_vae,\n",
        "                    'y_train': dataset['y_train'],\n",
        "                    'y_test': dataset['y_test'],\n",
        "                    'description': f\"{dataset['description']} + VAE features ({latent_dim}D)\"\n",
        "                }\n",
        "\n",
        "                print(f\"    âœ“ VAE features extracted: {latent_dim} dimensions\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    âŒ VAE failed for {name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return vae_datasets\n",
        "\n",
        "    def _build_vae(self, input_dim, latent_dim):\n",
        "        \"\"\"Build Variational Autoencoder\"\"\"\n",
        "        # Encoder\n",
        "        inputs = keras.Input(shape=(input_dim,))\n",
        "        h1 = layers.Dense(128, activation='relu')(inputs)\n",
        "        h1 = layers.BatchNormalization()(h1)\n",
        "        h1 = layers.Dropout(0.2)(h1)\n",
        "\n",
        "        h2 = layers.Dense(64, activation='relu')(h1)\n",
        "        h2 = layers.BatchNormalization()(h2)\n",
        "        h2 = layers.Dropout(0.2)(h2)\n",
        "\n",
        "        z_mean = layers.Dense(latent_dim, name='z_mean')(h2)\n",
        "        z_log_var = layers.Dense(latent_dim, name='z_log_var')(h2)\n",
        "\n",
        "        # Reparameterization trick\n",
        "        def sampling(args):\n",
        "            z_mean, z_log_var = args\n",
        "            batch = tf.shape(z_mean)[0]\n",
        "            dim = tf.shape(z_mean)[1]\n",
        "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "        z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "        # Decoder\n",
        "        decoder_h1 = layers.Dense(64, activation='relu')(z)\n",
        "        decoder_h1 = layers.BatchNormalization()(decoder_h1)\n",
        "\n",
        "        decoder_h2 = layers.Dense(128, activation='relu')(decoder_h1)\n",
        "        decoder_h2 = layers.BatchNormalization()(decoder_h2)\n",
        "\n",
        "        outputs = layers.Dense(input_dim, activation='sigmoid')(decoder_h2)\n",
        "\n",
        "        # VAE model\n",
        "        vae = keras.Model(inputs, outputs)\n",
        "\n",
        "        # VAE loss\n",
        "        reconstruction_loss = keras.losses.mse(inputs, outputs)\n",
        "        reconstruction_loss *= input_dim\n",
        "        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "        kl_loss = tf.reduce_mean(kl_loss) * -0.5\n",
        "        vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "        vae.add_loss(vae_loss)\n",
        "\n",
        "        vae.compile(optimizer='adam')\n",
        "        return vae\n",
        "\n",
        "    def train_models(self, datasets):\n",
        "        \"\"\"Train multiple models with comprehensive hyperparameter tuning\"\"\"\n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ¤– MODEL TRAINING WITH HYPERPARAMETER OPTIMIZATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Define model configurations\n",
        "        model_configs = {\n",
        "            'RandomForest': {\n",
        "                'model': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
        "                'params': {\n",
        "                    'n_estimators': [100, 200, 300, 500],\n",
        "                    'max_depth': [10, 20, 30, None],\n",
        "                    'min_samples_split': [2, 5, 10, 15],\n",
        "                    'min_samples_leaf': [1, 2, 4, 8],\n",
        "                    'max_features': ['sqrt', 'log2', None],\n",
        "                    'bootstrap': [True, False],\n",
        "                    'class_weight': ['balanced', 'balanced_subsample', None]\n",
        "                }\n",
        "            },\n",
        "            'XGBoost': {\n",
        "                'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "                'params': {\n",
        "                    'n_estimators': [100, 200, 300],\n",
        "                    'max_depth': [3, 6, 10],\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "                    'subsample': [0.8, 0.9, 1.0],\n",
        "                    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "                    'scale_pos_weight': [1, 3, 5]\n",
        "                }\n",
        "            },\n",
        "            'LightGBM': {\n",
        "                'model': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
        "                'params': {\n",
        "                    'n_estimators': [100, 200, 300],\n",
        "                    'max_depth': [3, 6, 10],\n",
        "                    'learning_rate': [0.01, 0.1, 0.2],\n",
        "                    'num_leaves': [31, 50, 100],\n",
        "                    'feature_fraction': [0.8, 0.9, 1.0],\n",
        "                    'bagging_fraction': [0.8, 0.9, 1.0],\n",
        "                    'scale_pos_weight': [1, 3, 5]\n",
        "                }\n",
        "            },\n",
        "            'ExtraTrees': {\n",
        "                'model': ExtraTreesClassifier(random_state=42, n_jobs=-1),\n",
        "                'params': {\n",
        "                    'n_estimators': [100, 200, 300],\n",
        "                    'max_depth': [10, 20, 30, None],\n",
        "                    'min_samples_split': [2, 5, 10],\n",
        "                    'min_samples_leaf': [1, 2, 4],\n",
        "                    'max_features': ['sqrt', 'log2', None],\n",
        "                    'class_weight': ['balanced', None]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for dataset_name, dataset in datasets.items():\n",
        "            print(f\"\\\\nðŸ“Š Processing {dataset_name} dataset...\")\n",
        "            print(f\"   Description: {dataset['description']}\")\n",
        "\n",
        "            results[dataset_name] = {}\n",
        "            X_train = dataset['X_train']\n",
        "            X_test = dataset['X_test']\n",
        "            y_train = dataset['y_train']\n",
        "            y_test = dataset['y_test']\n",
        "\n",
        "            for model_name, config in model_configs.items():\n",
        "                print(f\"\\\\n  ðŸ”§ Training {model_name}...\")\n",
        "\n",
        "                try:\n",
        "                    # Hyperparameter tuning\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    random_search = RandomizedSearchCV(\n",
        "                        estimator=config['model'],\n",
        "                        param_distributions=config['params'],\n",
        "                        n_iter=100,  # Reduced for faster execution\n",
        "                        cv=3,  # 3-fold CV for speed\n",
        "                        scoring='f1',\n",
        "                        random_state=42,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=0\n",
        "                    )\n",
        "\n",
        "                    random_search.fit(X_train, y_train)\n",
        "                    best_model = random_search.best_estimator_\n",
        "\n",
        "                    training_time = time.time() - start_time\n",
        "\n",
        "                    # Predictions\n",
        "                    y_pred = best_model.predict(X_test)\n",
        "                    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    metrics = {\n",
        "                        'accuracy': accuracy_score(y_test, y_pred),\n",
        "                        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "                        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "                        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "                        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
        "                    }\n",
        "\n",
        "                    results[dataset_name][model_name] = {\n",
        "                        'model': best_model,\n",
        "                        'best_params': random_search.best_params_,\n",
        "                        'best_cv_score': random_search.best_score_,\n",
        "                        'metrics': metrics,\n",
        "                        'predictions': y_pred,\n",
        "                        'probabilities': y_pred_proba,\n",
        "                        'y_test': y_test,\n",
        "                        'training_time': training_time\n",
        "                    }\n",
        "\n",
        "                    # Track best model\n",
        "                    if metrics['f1'] > self.best_score:\n",
        "                        self.best_score = metrics['f1']\n",
        "                        self.best_model = {\n",
        "                            'name': model_name,\n",
        "                            'dataset': dataset_name,\n",
        "                            'model': best_model,\n",
        "                            'metrics': metrics,\n",
        "                            'params': random_search.best_params_\n",
        "                        }\n",
        "\n",
        "                    print(f\"    âœ“ F1 Score: {metrics['f1']:.4f} (CV: {random_search.best_score_:.4f})\")\n",
        "                    print(f\"    â±ï¸ Training time: {training_time:.1f}s\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    âŒ Error training {model_name}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        self.results = results\n",
        "        print(f\"\\\\nðŸ† Best model: {self.best_model['name']} on {self.best_model['dataset']} (F1: {self.best_score:.4f})\")\n",
        "        return results\n",
        "\n",
        "    def create_comprehensive_visualizations(self):\n",
        "        \"\"\"Create comprehensive visualizations of results\"\"\"\n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ“Š CREATING COMPREHENSIVE VISUALIZATIONS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # 1. Model comparison heatmap\n",
        "        self._plot_model_comparison_heatmap()\n",
        "\n",
        "        # 2. Best model detailed analysis\n",
        "        self._plot_best_model_analysis()\n",
        "\n",
        "        # 3. Feature importance analysis\n",
        "        self._plot_feature_importance()\n",
        "\n",
        "        # 4. Model interpretation (SHAP/LIME)\n",
        "        if SHAP_AVAILABLE or LIME_AVAILABLE:\n",
        "            self._create_model_interpretation()\n",
        "\n",
        "    def _plot_model_comparison_heatmap(self):\n",
        "        \"\"\"Plot model comparison heatmap\"\"\"\n",
        "        print(\"Creating model comparison heatmap...\")\n",
        "\n",
        "        # Prepare data for heatmap\n",
        "        models = []\n",
        "        datasets = []\n",
        "        f1_scores = []\n",
        "\n",
        "        for dataset_name, dataset_results in self.results.items():\n",
        "            for model_name, result in dataset_results.items():\n",
        "                models.append(model_name)\n",
        "                datasets.append(dataset_name)\n",
        "                f1_scores.append(result['metrics']['f1'])\n",
        "\n",
        "        if not f1_scores:\n",
        "            print(\"No results available for comparison\")\n",
        "            return\n",
        "\n",
        "        # Create pivot table\n",
        "        comparison_df = pd.DataFrame({\n",
        "            'Model': models,\n",
        "            'Dataset': datasets,\n",
        "            'F1_Score': f1_scores\n",
        "        })\n",
        "\n",
        "        pivot_df = comparison_df.pivot(index='Model', columns='Dataset', values='F1_Score')\n",
        "\n",
        "        # Create heatmap\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(pivot_df, annot=True, cmap='YlOrRd', center=0.5,\n",
        "                   square=True, fmt='.3f', cbar_kws={\"shrink\": .8})\n",
        "        plt.title('Model Performance Comparison (F1 Scores)', fontsize=16, fontweight='bold')\n",
        "        plt.xlabel('Dataset', fontweight='bold')\n",
        "        plt.ylabel('Model', fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / 'plots' / 'model_comparison_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_best_model_analysis(self):\n",
        "        \"\"\"Create detailed analysis of the best model\"\"\"\n",
        "        if not self.best_model:\n",
        "            print(\"No best model available for detailed analysis\")\n",
        "            return\n",
        "\n",
        "        print(f\"Creating detailed analysis for best model: {self.best_model['name']}\")\n",
        "\n",
        "        # Get best model results\n",
        "        dataset_name = self.best_model['dataset']\n",
        "        model_name = self.best_model['name']\n",
        "        result = self.results[dataset_name][model_name]\n",
        "\n",
        "        y_test = result['y_test']\n",
        "        y_pred = result['predictions']\n",
        "        y_pred_proba = result['probabilities']\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "        fig.suptitle(f'Best Model Analysis: {model_name} on {dataset_name}', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Confusion Matrix\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
        "                   xticklabels=['No Dropout', 'Dropout'],\n",
        "                   yticklabels=['No Dropout', 'Dropout'])\n",
        "        axes[0, 0].set_title('Confusion Matrix')\n",
        "        axes[0, 0].set_ylabel('True Label')\n",
        "        axes[0, 0].set_xlabel('Predicted Label')\n",
        "\n",
        "        # 2. ROC Curve\n",
        "        from sklearn.metrics import roc_curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                       label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "        axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        axes[0, 1].set_xlim([0.0, 1.0])\n",
        "        axes[0, 1].set_ylim([0.0, 1.05])\n",
        "        axes[0, 1].set_xlabel('False Positive Rate')\n",
        "        axes[0, 1].set_ylabel('True Positive Rate')\n",
        "        axes[0, 1].set_title('ROC Curve')\n",
        "        axes[0, 1].legend(loc=\"lower right\")\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Prediction Distribution\n",
        "        axes[0, 2].hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='No Dropout', color='blue')\n",
        "        axes[0, 2].hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Dropout', color='red')\n",
        "        axes[0, 2].set_xlabel('Prediction Probability')\n",
        "        axes[0, 2].set_ylabel('Frequency')\n",
        "        axes[0, 2].set_title('Prediction Probability Distribution')\n",
        "        axes[0, 2].legend()\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Performance Metrics\n",
        "        metrics = result['metrics']\n",
        "        metric_names = list(metrics.keys())\n",
        "        metric_values = list(metrics.values())\n",
        "        bars = axes[1, 0].bar(metric_names, metric_values,\n",
        "                             color=['blue', 'green', 'orange', 'red', 'purple'])\n",
        "        axes[1, 0].set_title('Performance Metrics')\n",
        "        axes[1, 0].set_ylabel('Score')\n",
        "        axes[1, 0].set_ylim(0, 1.1)\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Add values on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                           f'{height:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 5. Feature Importance (if available)\n",
        "        model = result['model']\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "            indices = np.argsort(importances)[::-1][:15]  # Top 15 features\n",
        "\n",
        "            feature_names_short = [self.feature_names[i][:15] + '...'\n",
        "                                 if len(self.feature_names[i]) > 15\n",
        "                                 else self.feature_names[i] for i in indices]\n",
        "\n",
        "            axes[1, 1].barh(range(len(indices)), importances[indices], color='lightblue')\n",
        "            axes[1, 1].set_yticks(range(len(indices)))\n",
        "            axes[1, 1].set_yticklabels(feature_names_short, fontsize=8)\n",
        "            axes[1, 1].set_title('Top 15 Feature Importances')\n",
        "            axes[1, 1].set_xlabel('Importance')\n",
        "            axes[1, 1].invert_yaxis()\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'Feature importance\\\\nnot available',\n",
        "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "            axes[1, 1].set_title('Feature Importance')\n",
        "\n",
        "        # 6. Classification Report (as text)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        axes[1, 2].text(0.01, 0.99, report, transform=axes[1, 2].transAxes,\n",
        "                       fontsize=9, verticalalignment='top', fontfamily='monospace')\n",
        "        axes[1, 2].set_title('Classification Report')\n",
        "        axes[1, 2].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / 'plots' / 'best_model_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_feature_importance(self):\n",
        "        \"\"\"Plot comprehensive feature importance analysis\"\"\"\n",
        "        if not self.best_model or not hasattr(self.best_model['model'], 'feature_importances_'):\n",
        "            print(\"Feature importance not available for the best model\")\n",
        "            return\n",
        "\n",
        "        print(\"Creating feature importance analysis...\")\n",
        "\n",
        "        model = self.best_model['model']\n",
        "        importances = model.feature_importances_\n",
        "\n",
        "        # Create feature importance DataFrame\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Create visualizations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
        "        fig.suptitle('Feature Importance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # 1. Top 20 features\n",
        "        top_20 = feature_importance_df.head(20)\n",
        "        axes[0, 0].barh(range(len(top_20)), top_20['importance'], color='lightblue')\n",
        "        axes[0, 0].set_yticks(range(len(top_20)))\n",
        "        axes[0, 0].set_yticklabels([name[:20] + '...' if len(name) > 20 else name for name in top_20['feature']], fontsize=8)\n",
        "        axes[0, 0].set_title('Top 20 Most Important Features')\n",
        "        axes[0, 0].set_xlabel('Importance')\n",
        "        axes[0, 0].invert_yaxis()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # 2. Feature importance distribution\n",
        "        axes[0, 1].hist(importances, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "        axes[0, 1].set_xlabel('Feature Importance')\n",
        "        axes[0, 1].set_ylabel('Number of Features')\n",
        "        axes[0, 1].set_title('Distribution of Feature Importances')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # 3. Cumulative importance\n",
        "        cumulative_importance = np.cumsum(np.sort(importances)[::-1])\n",
        "        axes[1, 0].plot(range(1, len(cumulative_importance) + 1), cumulative_importance, 'b-', linewidth=2)\n",
        "        axes[1, 0].axhline(y=0.8, color='r', linestyle='--', label='80% Threshold')\n",
        "        axes[1, 0].axhline(y=0.9, color='orange', linestyle='--', label='90% Threshold')\n",
        "        axes[1, 0].set_xlabel('Number of Features')\n",
        "        axes[1, 0].set_ylabel('Cumulative Importance')\n",
        "        axes[1, 0].set_title('Cumulative Feature Importance')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # 4. Daily activity pattern (if applicable)\n",
        "        daily_features = feature_importance_df[feature_importance_df['feature'].str.contains('day_')]\n",
        "        if not daily_features.empty:\n",
        "            # Extract day numbers and activity types\n",
        "            daily_features['day'] = daily_features['feature'].str.extract(r'day_(\\\\d+)')[0].astype(float)\n",
        "            daily_features['activity'] = daily_features['feature'].str.extract(r'day_\\\\d+_(.+)')[0]\n",
        "\n",
        "            # Group by day and sum importances\n",
        "            daily_importance = daily_features.groupby('day')['importance'].sum().reset_index()\n",
        "\n",
        "            axes[1, 1].plot(daily_importance['day'], daily_importance['importance'], 'o-', linewidth=2, markersize=6)\n",
        "            axes[1, 1].set_xlabel('Day')\n",
        "            axes[1, 1].set_ylabel('Total Importance')\n",
        "            axes[1, 1].set_title('Feature Importance by Day')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "        else:\n",
        "            axes[1, 1].text(0.5, 0.5, 'No daily activity\\\\nfeatures found',\n",
        "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "            axes[1, 1].set_title('Daily Pattern Analysis')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.output_dir / 'plots' / 'feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        # Save feature importance data\n",
        "        feature_importance_df.to_csv(self.output_dir / 'reports' / 'feature_importance.csv', index=False)\n",
        "        print(f\"Feature importance saved to: {self.output_dir / 'reports' / 'feature_importance.csv'}\")\n",
        "\n",
        "    def _create_model_interpretation(self):\n",
        "        \"\"\"Create model interpretation using SHAP and LIME\"\"\"\n",
        "        if not self.best_model:\n",
        "            return\n",
        "\n",
        "        print(\"Creating model interpretation visualizations...\")\n",
        "\n",
        "        # Get best model data\n",
        "        dataset_name = self.best_model['dataset']\n",
        "        model_name = self.best_model['name']\n",
        "\n",
        "        # Find the dataset\n",
        "        dataset = None\n",
        "        for name, ds in self.prepare_datasets().items():\n",
        "            if name == dataset_name:\n",
        "                dataset = ds\n",
        "                break\n",
        "\n",
        "        if not dataset:\n",
        "            print(\"Could not find dataset for interpretation\")\n",
        "            return\n",
        "\n",
        "        model = self.best_model['model']\n",
        "        X_test_sample = dataset['X_test'][:100]  # Use sample for speed\n",
        "\n",
        "        # SHAP Analysis\n",
        "        if SHAP_AVAILABLE:\n",
        "            try:\n",
        "                print(\"  Creating SHAP visualizations...\")\n",
        "\n",
        "                # Create explainer based on model type\n",
        "                if hasattr(model, 'tree_'):\n",
        "                    explainer = shap.TreeExplainer(model)\n",
        "                else:\n",
        "                    explainer = shap.Explainer(model, X_test_sample)\n",
        "\n",
        "                shap_values = explainer.shap_values(X_test_sample)\n",
        "\n",
        "                # Handle different SHAP value formats\n",
        "                if isinstance(shap_values, list):\n",
        "                    shap_values = shap_values[1]  # For binary classification\n",
        "\n",
        "                # Summary plot\n",
        "                plt.figure()\n",
        "                shap.summary_plot(shap_values, X_test_sample,\n",
        "                                feature_names=self.feature_names, show=False, max_display=20)\n",
        "                plt.title(\"SHAP Summary Plot (Top 20 Features)\")\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(self.output_dir / 'plots' / 'shap_summary.png', dpi=300, bbox_inches='tight')\n",
        "                plt.show()\n",
        "\n",
        "                # Waterfall plot for first instance\n",
        "                if hasattr(shap, 'waterfall_plot'):\n",
        "                    plt.figure()\n",
        "                    shap.waterfall_plot(explainer.expected_value, shap_values[0], X_test_sample[0],\n",
        "                                      feature_names=self.feature_names, max_display=15, show=False)\n",
        "                    plt.title(\"SHAP Waterfall Plot (First Instance)\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(self.output_dir / 'plots' / 'shap_waterfall.png', dpi=300, bbox_inches='tight')\n",
        "                    plt.show()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  SHAP analysis failed: {e}\")\n",
        "\n",
        "        # LIME Analysis\n",
        "        if LIME_AVAILABLE:\n",
        "            try:\n",
        "                print(\"  Creating LIME visualizations...\")\n",
        "\n",
        "                # Create LIME explainer\n",
        "                explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                    dataset['X_train'],\n",
        "                    feature_names=self.feature_names,\n",
        "                    class_names=['No Dropout', 'Dropout'],\n",
        "                    mode='classification'\n",
        "                )\n",
        "\n",
        "                # Explain a few instances\n",
        "                for idx in [0, 1, 2]:\n",
        "                    if idx >= len(X_test_sample):\n",
        "                        break\n",
        "\n",
        "                    exp = explainer.explain_instance(\n",
        "                        X_test_sample[idx],\n",
        "                        model.predict_proba,\n",
        "                        num_features=15,\n",
        "                        top_labels=2\n",
        "                    )\n",
        "\n",
        "                    fig = exp.as_pyplot_figure()\n",
        "                    plt.title(f\"LIME Explanation - Instance {idx}\")\n",
        "                    plt.tight_layout()\n",
        "                    plt.savefig(self.output_dir / 'plots' / f'lime_explanation_{idx}.png', dpi=300, bbox_inches='tight')\n",
        "                    plt.show()\n",
        "\n",
        "                    # Save HTML report\n",
        "                    exp.save_to_file(self.output_dir / 'reports' / f'lime_explanation_{idx}.html')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  LIME analysis failed: {e}\")\n",
        "\n",
        "    def save_best_model(self):\n",
        "        \"\"\"Save the best model and related components\"\"\"\n",
        "        if not self.best_model:\n",
        "            print(\"No best model to save\")\n",
        "            return\n",
        "\n",
        "        print(\"\\\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ’¾ SAVING BEST MODEL AND COMPONENTS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Save model\n",
        "        model_path = self.output_dir / 'models' / f'best_model_{timestamp}.joblib'\n",
        "        joblib.dump(self.best_model['model'], model_path)\n",
        "        print(f\"âœ“ Model saved: {model_path}\")\n",
        "\n",
        "        # Save scaler\n",
        "        scaler_path = self.output_dir / 'models' / f'scaler_{timestamp}.joblib'\n",
        "        joblib.dump(self.scaler, scaler_path)\n",
        "        print(f\"âœ“ Scaler saved: {scaler_path}\")\n",
        "\n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'model_name': self.best_model['name'],\n",
        "            'dataset': self.best_model['dataset'],\n",
        "            'metrics': self.best_model['metrics'],\n",
        "            'hyperparameters': self.best_model['params'],\n",
        "            'feature_names': self.feature_names,\n",
        "            'best_score': self.best_score,\n",
        "            'timestamp': timestamp,\n",
        "            'model_path': str(model_path),\n",
        "            'scaler_path': str(scaler_path)\n",
        "        }\n",
        "\n",
        "        metadata_path = self.output_dir / 'models' / f'model_metadata_{timestamp}.pkl'\n",
        "        with open(metadata_path, 'wb') as f:\n",
        "            pickle.dump(metadata, f)\n",
        "        print(f\"âœ“ Metadata saved: {metadata_path}\")\n",
        "\n",
        "        # Create results summary\n",
        "        self._create_results_summary(timestamp)\n",
        "\n",
        "        print(f\"\\\\nðŸŽ‰ Best model summary:\")\n",
        "        print(f\"   Model: {self.best_model['name']}\")\n",
        "        print(f\"   Dataset: {self.best_model['dataset']}\")\n",
        "        print(f\"   F1 Score: {self.best_score:.4f}\")\n",
        "        print(f\"   All files saved in: {self.output_dir.absolute()}\")\n",
        "\n",
        "        return model_path, scaler_path, metadata_path\n",
        "\n",
        "    def _create_results_summary(self, timestamp):\n",
        "        \"\"\"Create comprehensive results summary\"\"\"\n",
        "        summary = {\n",
        "            'execution_timestamp': timestamp,\n",
        "            'best_model': self.best_model,\n",
        "            'all_results': {}\n",
        "        }\n",
        "\n",
        "        # Add all results with simplified structure\n",
        "        for dataset_name, dataset_results in self.results.items():\n",
        "            summary['all_results'][dataset_name] = {}\n",
        "            for model_name, result in dataset_results.items():\n",
        "                summary['all_results'][dataset_name][model_name] = {\n",
        "                    'metrics': result['metrics'],\n",
        "                    'best_params': result['best_params'],\n",
        "                    'best_cv_score': result['best_cv_score'],\n",
        "                    'training_time': result['training_time']\n",
        "                }\n",
        "\n",
        "        # Save as JSON\n",
        "        import json\n",
        "        summary_path = self.output_dir / 'reports' / f'results_summary_{timestamp}.json'\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2, default=str)\n",
        "\n",
        "        print(f\"âœ“ Results summary saved: {summary_path}\")\n",
        "\n",
        "        # Create CSV for easy viewing\n",
        "        results_list = []\n",
        "        for dataset_name, dataset_results in self.results.items():\n",
        "            for model_name, result in dataset_results.items():\n",
        "                row = {\n",
        "                    'dataset': dataset_name,\n",
        "                    'model': model_name,\n",
        "                    'training_time': result['training_time'],\n",
        "                    **result['metrics']\n",
        "                }\n",
        "                results_list.append(row)\n",
        "\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "        results_csv_path = self.output_dir / 'reports' / f'results_comparison_{timestamp}.csv'\n",
        "        results_df.to_csv(results_csv_path, index=False)\n",
        "        print(f\"âœ“ Results comparison saved: {results_csv_path}\")\n",
        "\n",
        "    def run_complete_pipeline(self):\n",
        "        \"\"\"Run the complete enhanced ML pipeline\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(\"ðŸš€\" * 20)\n",
        "        print(\"ENHANCED MACHINE LEARNING PIPELINE FOR DROPOUT PREDICTION\")\n",
        "        print(\"ðŸš€\" * 20)\n",
        "        print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load and explore data\n",
        "            self.load_and_explore_data()\n",
        "\n",
        "            # Step 2: Prepare datasets with different sampling strategies\n",
        "            datasets = self.prepare_datasets()\n",
        "\n",
        "            # Step 3: Train models with hyperparameter tuning\n",
        "            self.train_models(datasets)\n",
        "\n",
        "            # Step 4: Create comprehensive visualizations\n",
        "            self.create_comprehensive_visualizations()\n",
        "\n",
        "            # Step 5: Save best model and create reports\n",
        "            self.save_best_model()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Pipeline failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\\\n\" + \"ðŸŽ‰\" * 20)\n",
        "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "        print(\"ðŸŽ‰\" * 20)\n",
        "        print(f\"Total execution time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"Best model: {self.best_model['name']} on {self.best_model['dataset']}\")\n",
        "        print(f\"Best F1 Score: {self.best_score:.4f}\")\n",
        "        print(f\"All results saved in: {self.output_dir.absolute()}\")\n",
        "\n",
        "        return self.results, self.best_model\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    # Check if data file exists\n",
        "    data_file = 'model1_210_features_spliting.csv'\n",
        "    if not os.path.exists(data_file):\n",
        "        print(f\"âŒ Data file '{data_file}' not found!\")\n",
        "        print(\"Please ensure the CSV file is in the current directory.\")\n",
        "        return\n",
        "\n",
        "    # Initialize and run pipeline\n",
        "    pipeline = CompleteMLPipeline(data_path=data_file)\n",
        "    results, best_model = pipeline.run_complete_pipeline()\n",
        "\n",
        "    return pipeline, results, best_model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    pipeline, results, best_model = main()"
      ]
    }
  ]
}