{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuXI+y1ozvsyPgJ5o7OMzP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aymenchibouti/newversion/blob/master/best_tuning_model_XAI_res.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap\n",
        "!pip install lime\n",
        "!pip install xgboost\n",
        "!pip install imblearn\n",
        "!pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUxNYnbCaZgi",
        "outputId": "332a88cd-635f-4b2c-b2a2-bb7409e8e83a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=4665bccab2764e9999a225db3e3372f20a269644bc5b79a74689b89ed0f86ee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.1)\n",
            "Collecting imblearn\n",
            "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.6.0)\n",
            "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
            "Installing collected packages: imblearn\n",
            "Successfully installed imblearn-0.0\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.11/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhFbEAGDXeVH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'model1_210_features.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop non-numeric columns that are not useful for prediction\n",
        "data = data.drop(columns=['username', 'course_id', 'enrollment_id'])\n",
        "\n",
        "# Handle missing values (fill with 0 or use mean/median imputation as necessary)\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['dropout'])  # Features\n",
        "y = data['dropout']  # Target variable\n",
        "\n",
        "# Standardize the features (important for models like Logistic Regression and XGBoost)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Handle class imbalance using SMOTE (oversampling the minority class)\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# RandomizedSearchCV for hyperparameter tuning (for Random Forest)\n",
        "param_dist_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf_random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist_rf, n_iter=100, cv=3, random_state=42)\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "\n",
        "# XGBoost RandomizedSearchCV (hyperparameter tuning for XGBoost)\n",
        "param_dist_xgb = {\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(random_state=42)\n",
        "xgb_random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_dist_xgb, n_iter=100, cv=3, random_state=42)\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "\n",
        "# Logistic Regression with class weight to handle imbalance\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Best models from RandomizedSearchCV\n",
        "best_rf = rf_random_search.best_estimator_\n",
        "best_xgb = xgb_random_search.best_estimator_\n",
        "\n",
        "# Predictions and evaluation for each model\n",
        "models = [lr, best_rf, best_xgb]\n",
        "model_names = ['Logistic Regression', 'Random Forest (Tuned)', 'XGBoost (Tuned)']\n",
        "\n",
        "for model, name in zip(models, model_names):\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Evaluation for {name}:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Explainability using SHAP (SHAP can work with tree-based models like RandomForest and XGBoost)\n",
        "explainer_rf = shap.TreeExplainer(best_rf)\n",
        "shap_values_rf = explainer_rf.shap_values(X_test)\n",
        "\n",
        "# SHAP summary plot for Random Forest\n",
        "shap.summary_plot(shap_values_rf, X_test, feature_names=X.columns)\n",
        "\n",
        "# LIME - Local Interpretable Model-Agnostic Explanations\n",
        "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train,\n",
        "    training_labels=y_train,\n",
        "    mode=\"classification\",\n",
        "    feature_names=X.columns,\n",
        "    class_names=[\"No Dropout\", \"Dropout\"],\n",
        "    verbose=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Pick a single instance for LIME explanation\n",
        "instance = X_test[0]  # First instance in the test set\n",
        "explanation_lime = explainer_lime.explain_instance(instance, best_xgb.predict_proba)\n",
        "\n",
        "# Visualize LIME explanation\n",
        "explanation_lime.show_in_notebook()\n",
        "\n",
        "# Feature Importance (using Random Forest)\n",
        "feature_importance = best_rf.feature_importances_\n",
        "# Visualize the feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(X.columns, feature_importance)\n",
        "plt.title(\"Feature Importance (Random Forest - Tuned)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAOu0YMRejKq",
        "outputId": "ed44cdb0-1f61-4844-af95-adb88ead083c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'model1_210_features.csv'  # Update with your file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"Target distribution:\\n{data['dropout'].value_counts()}\")\n",
        "print(f\"Class distribution: {data['dropout'].value_counts(normalize=True)}\")\n",
        "\n",
        "# Drop non-numeric columns that are not useful for prediction\n",
        "data = data.drop(columns=['username', 'course_id', 'enrollment_id'])\n",
        "\n",
        "# Handle missing values\n",
        "data.fillna(0, inplace=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop(columns=['dropout'])\n",
        "y = data['dropout']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Feature names: {list(X.columns[:10])}...\")  # Show first 10 features\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Handle class imbalance - try different resampling strategies\n",
        "print(\"\\n=== Testing Different Resampling Strategies ===\")\n",
        "\n",
        "# 1. SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_smote, y_smote = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# 2. ADASYN\n",
        "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
        "X_adasyn, y_adasyn = adasyn.fit_resample(X_scaled, y)\n",
        "\n",
        "# 3. SMOTETomek (combination of over and under sampling)\n",
        "smotetomek = SMOTETomek(sampling_strategy='auto', random_state=42)\n",
        "X_smotetomek, y_smotetomek = smotetomek.fit_resample(X_scaled, y)\n",
        "\n",
        "# Choose the best resampling strategy (we'll use SMOTE for now)\n",
        "X_resampled, y_resampled = X_smote, y_smote\n",
        "print(f\"After resampling: {X_resampled.shape}, Target distribution: {np.bincount(y_resampled)}\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
        ")\n",
        "\n",
        "# Cross-validation strategy\n",
        "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "print(\"\\n=== Model Training and Hyperparameter Tuning ===\")\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# 1. XGBoost with extensive hyperparameter tuning\n",
        "print(\"1. Training XGBoost...\")\n",
        "xgb_params = {\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [1, 1.5, 2]\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=xgb_params,\n",
        "    n_iter=200,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "results['XGBoost'] = xgb_random_search.best_estimator_\n",
        "\n",
        "print(f\"Best XGBoost params: {xgb_random_search.best_params_}\")\n",
        "print(f\"Best XGBoost CV score: {xgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# 2. LightGBM\n",
        "print(\"2. Training LightGBM...\")\n",
        "lgb_params = {\n",
        "    'num_leaves': [31, 50, 70, 100],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
        "    'feature_fraction': [0.8, 0.9, 1.0],\n",
        "    'bagging_fraction': [0.8, 0.9, 1.0],\n",
        "    'bagging_freq': [1, 5, 10],\n",
        "    'min_child_samples': [5, 10, 20],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.1, 0.5]\n",
        "}\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "lgb_random_search = RandomizedSearchCV(\n",
        "    estimator=lgb_model,\n",
        "    param_distributions=lgb_params,\n",
        "    n_iter=150,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgb_random_search.fit(X_train, y_train)\n",
        "results['LightGBM'] = lgb_random_search.best_estimator_\n",
        "\n",
        "print(f\"Best LightGBM params: {lgb_random_search.best_params_}\")\n",
        "print(f\"Best LightGBM CV score: {lgb_random_search.best_score_:.4f}\")\n",
        "\n",
        "# 3. CatBoost\n",
        "print(\"3. Training CatBoost...\")\n",
        "catboost_params = {\n",
        "    'depth': [4, 6, 8, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
        "    'iterations': [100, 200, 300, 500],\n",
        "    'l2_leaf_reg': [1, 3, 5, 7],\n",
        "    'border_count': [32, 64, 128],\n",
        "    'random_strength': [1, 2, 3]\n",
        "}\n",
        "\n",
        "catboost_model = cb.CatBoostClassifier(random_state=42, verbose=False)\n",
        "catboost_random_search = RandomizedSearchCV(\n",
        "    estimator=catboost_model,\n",
        "    param_distributions=catboost_params,\n",
        "    n_iter=100,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "catboost_random_search.fit(X_train, y_train)\n",
        "results['CatBoost'] = catboost_random_search.best_estimator_\n",
        "\n",
        "print(f\"Best CatBoost params: {catboost_random_search.best_params_}\")\n",
        "print(f\"Best CatBoost CV score: {catboost_random_search.best_score_:.4f}\")\n",
        "\n",
        "# 4. Random Forest with extensive tuning\n",
        "print(\"4. Training Random Forest...\")\n",
        "rf_params = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10, 15],\n",
        "    'min_samples_leaf': [1, 2, 4, 8],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_random_search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=rf_params,\n",
        "    n_iter=150,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_random_search.fit(X_train, y_train)\n",
        "results['Random Forest'] = rf_random_search.best_estimator_\n",
        "\n",
        "print(f\"Best Random Forest params: {rf_random_search.best_params_}\")\n",
        "print(f\"Best Random Forest CV score: {rf_random_search.best_score_:.4f}\")\n",
        "\n",
        "# 5. Extra Trees\n",
        "print(\"5. Training Extra Trees...\")\n",
        "et_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "et_model = ExtraTreesClassifier(random_state=42)\n",
        "et_random_search = RandomizedSearchCV(\n",
        "    estimator=et_model,\n",
        "    param_distributions=et_params,\n",
        "    n_iter=100,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "et_random_search.fit(X_train, y_train)\n",
        "results['Extra Trees'] = et_random_search.best_estimator_\n",
        "\n",
        "# 6. Gradient Boosting\n",
        "print(\"6. Training Gradient Boosting...\")\n",
        "gb_params = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_random_search = RandomizedSearchCV(\n",
        "    estimator=gb_model,\n",
        "    param_distributions=gb_params,\n",
        "    n_iter=100,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "gb_random_search.fit(X_train, y_train)\n",
        "results['Gradient Boosting'] = gb_random_search.best_estimator_\n",
        "\n",
        "# 7. SVM\n",
        "print(\"7. Training SVM...\")\n",
        "svm_params = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "svm_model = SVC(random_state=42, probability=True)\n",
        "svm_random_search = RandomizedSearchCV(\n",
        "    estimator=svm_model,\n",
        "    param_distributions=svm_params,\n",
        "    n_iter=50,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "svm_random_search.fit(X_train, y_train)\n",
        "results['SVM'] = svm_random_search.best_estimator_\n",
        "\n",
        "# 8. Neural Network (sklearn)\n",
        "print(\"8. Training MLP Classifier...\")\n",
        "mlp_params = {\n",
        "    'hidden_layer_sizes': [(100,), (200,), (100, 50), (200, 100), (300, 200, 100)],\n",
        "    'activation': ['tanh', 'relu'],\n",
        "    'solver': ['adam', 'lbfgs'],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate': ['constant', 'adaptive'],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "mlp_model = MLPClassifier(random_state=42)\n",
        "mlp_random_search = RandomizedSearchCV(\n",
        "    estimator=mlp_model,\n",
        "    param_distributions=mlp_params,\n",
        "    n_iter=50,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "mlp_random_search.fit(X_train, y_train)\n",
        "results['MLP'] = mlp_random_search.best_estimator_\n",
        "\n",
        "# 9. Deep Learning with Keras/TensorFlow\n",
        "print(\"9. Training Deep Neural Network...\")\n",
        "\n",
        "def create_dnn_model(input_dim, dropout_rate=0.3, l1_reg=0.01, l2_reg=0.01):\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_dim=input_dim,\n",
        "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(256, activation='relu',\n",
        "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(128, activation='relu',\n",
        "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(64, activation='relu',\n",
        "              kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)),\n",
        "        Dropout(dropout_rate),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Best DNN model after some manual tuning\n",
        "dnn_model = create_dnn_model(X_train.shape[1], dropout_rate=0.4, l1_reg=0.001, l2_reg=0.01)\n",
        "dnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=8, min_lr=0.0001)\n",
        "\n",
        "# Train DNN\n",
        "history = dnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=100,\n",
        "    batch_size=128,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "results['Deep Neural Network'] = dnn_model\n",
        "\n",
        "print(\"\\n=== Model Evaluation ===\")\n",
        "\n",
        "# Evaluate all models\n",
        "evaluation_results = []\n",
        "\n",
        "for name, model in results.items():\n",
        "    if name == 'Deep Neural Network':\n",
        "        # For Keras model\n",
        "        y_pred_proba = model.predict(X_test)\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "    else:\n",
        "        # For sklearn models\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    evaluation_results.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'AUC-ROC': auc\n",
        "    })\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "    print(f\"  F1-Score:  {f1:.4f}\")\n",
        "    print(f\"  AUC-ROC:   {auc:.4f}\")\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(evaluation_results)\n",
        "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n=== Final Rankings ===\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Accuracy comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.barh(results_df['Model'], results_df['Accuracy'])\n",
        "plt.title('Model Accuracy Comparison')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.xlim(0, 1)\n",
        "for i, v in enumerate(results_df['Accuracy']):\n",
        "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
        "\n",
        "# F1-Score comparison\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.barh(results_df['Model'], results_df['F1-Score'])\n",
        "plt.title('Model F1-Score Comparison')\n",
        "plt.xlabel('F1-Score')\n",
        "plt.xlim(0, 1)\n",
        "for i, v in enumerate(results_df['F1-Score']):\n",
        "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
        "\n",
        "# AUC-ROC comparison\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.barh(results_df['Model'], results_df['AUC-ROC'])\n",
        "plt.title('Model AUC-ROC Comparison')\n",
        "plt.xlabel('AUC-ROC')\n",
        "plt.xlim(0, 1)\n",
        "for i, v in enumerate(results_df['AUC-ROC']):\n",
        "    plt.text(v + 0.01, i, f'{v:.4f}', va='center')\n",
        "\n",
        "# All metrics heatmap\n",
        "plt.subplot(2, 2, 4)\n",
        "metrics_for_heatmap = results_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']]\n",
        "sns.heatmap(metrics_for_heatmap, annot=True, cmap='viridis', fmt='.4f')\n",
        "plt.title('All Metrics Heatmap')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance for the best tree-based model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "if best_model_name in ['XGBoost', 'LightGBM', 'CatBoost', 'Random Forest', 'Extra Trees', 'Gradient Boosting']:\n",
        "    best_model = results[best_model_name]\n",
        "\n",
        "    if hasattr(best_model, 'feature_importances_'):\n",
        "        feature_importance = best_model.feature_importances_\n",
        "    elif hasattr(best_model, 'get_feature_importance'):  # CatBoost\n",
        "        feature_importance = best_model.get_feature_importance()\n",
        "    else:\n",
        "        feature_importance = None\n",
        "\n",
        "    if feature_importance is not None:\n",
        "        # Get top 20 most important features\n",
        "        feature_names = X.columns\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': feature_importance\n",
        "        }).sort_values('importance', ascending=False).head(20)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.barh(importance_df['feature'], importance_df['importance'])\n",
        "        plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
        "        plt.xlabel('Importance')\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "print(f\"\\n=== Best Model: {results_df.iloc[0]['Model']} ===\")\n",
        "print(f\"Best Accuracy: {results_df.iloc[0]['Accuracy']:.4f}\")\n",
        "print(f\"Best F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
        "print(f\"Best AUC-ROC: {results_df.iloc[0]['AUC-ROC']:.4f}\")\n",
        "\n",
        "# Save the best model\n",
        "best_model = results[results_df.iloc[0]['Model']]\n",
        "if results_df.iloc[0]['Model'] != 'Deep Neural Network':\n",
        "    import joblib\n",
        "    joblib.dump(best_model, f'best_model_{results_df.iloc[0][\"Model\"].replace(\" \", \"_\").lower()}.pkl')\n",
        "    print(f\"\\nBest model saved as: best_model_{results_df.iloc[0]['Model'].replace(' ', '_').lower()}.pkl\")\n",
        "else:\n",
        "    best_model.save('best_model_deep_neural_network.h5')\n",
        "    print(f\"\\nBest model saved as: best_model_deep_neural_network.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAbTdYA-eXVl",
        "outputId": "f42c8c48-18c7-4f6e-ebfd-b19eb9952f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (120542, 214)\n",
            "Target distribution:\n",
            "dropout\n",
            "1    95581\n",
            "0    24961\n",
            "Name: count, dtype: int64\n",
            "Class distribution: dropout\n",
            "1    0.792927\n",
            "0    0.207073\n",
            "Name: proportion, dtype: float64\n",
            "Features shape: (120542, 210)\n",
            "Feature names: ['day_1_access', 'day_1_problem', 'day_1_wiki', 'day_1_discussion', 'day_1_navigate', 'day_1_page_close', 'day_1_video', 'day_2_access', 'day_2_problem', 'day_2_wiki']...\n",
            "\n",
            "=== Testing Different Resampling Strategies ===\n",
            "After resampling: (191162, 210), Target distribution: [95581 95581]\n",
            "\n",
            "=== Model Training and Hyperparameter Tuning ===\n",
            "1. Training XGBoost...\n",
            "Best XGBoost params: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.5, 'n_estimators': 500, 'min_child_weight': 1, 'max_depth': 7, 'learning_rate': 0.15, 'gamma': 0.1, 'colsample_bytree': 1.0}\n",
            "Best XGBoost CV score: 0.8805\n",
            "2. Training LightGBM...\n",
            "Best LightGBM params: {'reg_lambda': 0, 'reg_alpha': 0.5, 'num_leaves': 100, 'n_estimators': 300, 'min_child_samples': 20, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_freq': 1, 'bagging_fraction': 0.8}\n",
            "Best LightGBM CV score: 0.8836\n",
            "3. Training CatBoost...\n"
          ]
        }
      ]
    }
  ]
}